{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SampleDiseaseOutbreakPrediction.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "y6KBCWoelqcN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Explore Test Data and Run Saved Logistics Regression Model to predict outbreak of flu\n",
        "\n",
        "import org.apache.spark.sql.Dataset\n",
        "import org.apache.spark.sql.Encoders\n",
        "import org.apache.spark.sql.Row\n",
        "import org.apache.spark.sql.SparkSession\n",
        "import org.apache.spark.sql.streaming.StreamingQuery\n",
        "import org.apache.spark.sql.streaming.StreamingQueryException\n",
        "import org.apache.spark.sql.types.StructType\n",
        "\n",
        "val tweetSchema = new StructType()\n",
        "                .add(\"tweetId\", \"string\")\n",
        "                .add(\"tweetText\", \"string\")\n",
        "                .add(\"location\", \"string\")\n",
        "                .add(\"timestamp\", \"string\");\n",
        "\n",
        "val spark = SparkSession\n",
        "\t\t.builder()\n",
        "\t\t.appName(\"StreamHandler\")\n",
        "\t\t.config(\"spark.master\", \"local\")\n",
        "\t\t.getOrCreate();\n",
        "            \n",
        "import org.apache.spark.sql.types._\n",
        "import org.apache.spark.sql.functions.{unix_timestamp, to_date}\n",
        "\n",
        "val realTweets = \"/home/opt/data/test/\"\n",
        "\n",
        "val tweetStream = spark.readStream\n",
        "\t\t       .schema(tweetSchema)\n",
        "\t\t       .option(\"maxFilesPerTrigger\", 1)\n",
        "\t\t       .json(realTweets)\n",
        "\t\t       .select($\"location\", to_date(unix_timestamp($\"timestamp\", \"EEE MMM dd HH:mm:ss Z yyyy\").cast(\"timestamp\")).as(\"timestamp\"))\n",
        " \n",
        " val streamingCountsDF = \n",
        "  tweetStream\n",
        "    .groupBy($\"location\", $\"timestamp\" , window($\"timestamp\", \"1 hour\"))\n",
        "    .count()\n",
        "    \n",
        "streamingCountsDF.isStreaming\n",
        "\n",
        "spark.conf.set(\"spark.sql.shuffle.partitions\", \"1\") \n",
        "\n",
        "import org.apache.spark.sql.functions._\n",
        "import org.apache.spark.sql.streaming._\n",
        "import org.apache.spark.sql.types._\n",
        "\n",
        "val query =\n",
        "  streamingCountsDF\n",
        "    .writeStream\n",
        "    .format(\"memory\")     \n",
        "    .trigger(ProcessingTime(\"10 seconds\"))\n",
        "    .queryName(\"testTable2\")     \n",
        "    .outputMode(\"complete\") \n",
        "    .start()\n",
        "\n",
        "%sql\n",
        "select location, weekofyear(window.end) as Week, AVG(count) \n",
        "as weekly_avg from testTable2 where count > 0 group by location,  weekofyear(window.end) \n",
        "having count(*) > 0 order by weekly_avg DESC\n",
        "\n",
        "\n",
        "%sql\n",
        "select location, date_format(window.end, \"dd-MM-YYYY\") as time, AVG(count) \n",
        "as count from testTable2 where count > 0 group by location,  date_format(window.end, \"dd-MM-YYYY\") \n",
        "having count(*) > 0 order by count DESC\n",
        "\n",
        "\n",
        "%sql\n",
        "select weekofyear(window.end) as Week from testTable2 where count > 0 group by  weekofyear(timestamp)\n",
        "\n",
        "// create the features for test data and run the prediction\n",
        "import org.apache.spark.sql.functions.udf\n",
        "import org.apache.spark.ml.feature.VectorAssembler\n",
        "import org.apache.spark.ml.linalg.DenseVector\n",
        "import org.apache.spark.ml.linalg.Vectors\n",
        "import org.apache.spark.ml.linalg._\n",
        "import org.apache.spark.ml.classification.BinaryLogisticRegressionSummary\n",
        "import org.apache.spark.ml.evaluation.BinaryClassificationEvaluator\n",
        "import org.apache.spark.ml.classification.LogisticRegression\n",
        "import org.apache.spark.ml.feature.StringIndexer\n",
        "import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics\n",
        "import org.apache.spark.mllib.tree.RandomForest\n",
        "import org.apache.spark.mllib.tree.model.RandomForestModel\n",
        "import org.apache.spark.mllib.util.MLUtils\n",
        "import org.apache.spark.mllib.regression.LabeledPoint\n",
        "import org.apache.spark.ml.{Pipeline, PipelineModel}\n",
        "\n",
        "////\n",
        "val locationRankDF = sql(\"select row_number() over (order by location) as locRank, location as locName from testTable2  GROUP BY location\")\n",
        "locationRankDF.createOrReplaceTempView(\"locationRankMap\")\n",
        "println(\"Location Rank\")\n",
        "\n",
        "//\n",
        "def calcLabel: (Double => Double) = (arg: Double) => {if (arg > 2.5) 1.0 else 0.0 }\n",
        "\n",
        "// TESTING DATA SET\n",
        "\n",
        "val testDf1 = sql(s\"\"\"\n",
        "SELECT locRank, avg(count) as weeklyAvg, weekofyear(window.end) as week\n",
        "  FROM testTable2\n",
        "  JOIN locationRankMap ON location = locName\n",
        "  GROUP BY weekofyear(window.end), locRank\n",
        "  HAVING weeklyAvg > 0\n",
        "  order by weeklyAvg DESC\n",
        "  \"\"\")\n",
        "  \n",
        "//df1.show\n",
        "\n",
        "val testDf2 = testDf1.select($\"locRank\".cast(\"Double\"), $\"weeklyAvg\".cast(\"Double\"), $\"week\".cast(\"Double\"))\n",
        "val flulabel = udf(calcLabel)\n",
        "\n",
        "val testDf3 = testDf2.withColumn(\"class\", flulabel(testDf2(\"weeklyAvg\")))\n",
        "val assembler = new VectorAssembler()\n",
        "  .setInputCols(Array(\"locRank\", \"weeklyAvg\",\"week\"))\n",
        "  .setOutputCol(\"features\")\n",
        "\n",
        "val testDf4 = assembler.transform(testDf3)\n",
        "\n",
        "val labelIndexer = new StringIndexer().setInputCol(\"class\").setOutputCol(\"label\")\n",
        "val testDf5 = labelIndexer.fit(testDf4).transform(testDf4)\n",
        "\n",
        "val splitSeed = 5043\n",
        "val Array(holdoutData2, testData) = testDf5.randomSplit(Array(0.0, 1.0), splitSeed)\n",
        "\n",
        "testData.show\n",
        "\n",
        "//////////////////////////////// ///////////////////////////////////// ////////////////////////////////////////\n",
        "\n",
        "val model = PipelineModel.load(\"/home/opt/models/lr-model4\")\n",
        "\n",
        "//println(s\"Coefficients: ${model.coefficients} Intercept: ${model.intercept}\")\n",
        "\n",
        "val predictions = model.transform(testData)\n",
        "\n",
        "print(\"$$$$$$ predictions size >>> \"+predictions.count())\n",
        "\n",
        "predictions.show\n",
        "\n",
        "val evaluator = new BinaryClassificationEvaluator().setLabelCol(\"label\").setRawPredictionCol(\"rawPrediction\").setMetricName(\"areaUnderROC\")\n",
        "val accuracy = evaluator.evaluate(predictions)\n",
        "\n",
        "val lp = predictions.select( \"label\", \"prediction\")\n",
        "val counttotal = predictions.count()\n",
        "val correct = lp.filter($\"label\" === $\"prediction\").count()\n",
        "val wrong = lp.filter(not($\"label\" === $\"prediction\")).count()\n",
        "val truep = lp.filter($\"prediction\" === 0.0).filter($\"label\" === $\"prediction\").count()\n",
        "val falseN = lp.filter($\"prediction\" === 0.0).filter(not($\"label\" === $\"prediction\")).count()\n",
        "val falseP = lp.filter($\"prediction\" === 1.0).filter(not($\"label\" === $\"prediction\")).count()\n",
        "val ratioWrong=wrong.toDouble/counttotal.toDouble\n",
        "val ratioCorrect=correct.toDouble/counttotal.toDouble\n",
        "\n",
        "val  predictionAndLabels =predictions.select(\"rawPrediction\", \"label\").rdd.map(x => (x(0).asInstanceOf[DenseVector](1), x(1).asInstanceOf[Double]))\n",
        "val metrics = new BinaryClassificationMetrics(predictionAndLabels)\n",
        "println(\"area under the precision-recall curve: \" + metrics.areaUnderPR)\n",
        "println(\"area under the receiver operating characteristic (ROC) curve : \" + metrics.areaUnderROC)\n",
        "\n",
        "\n",
        "//accuracy: Double = 0.941260162601624\n",
        "//lp: org.apache.spark.sql.DataFrame = [label: double, prediction: double]\n",
        "//counttotal: Long = 229\n",
        "//correct: Long = 220\n",
        "//wrong: Long = 9\n",
        "//truep: Long = 203\n",
        "//falseN: Long = 7\n",
        "//falseP: Long = 2\n",
        "//ratioWrong: Double = 0.039301310043668124\n",
        "//ratioCorrect: Double = 0.9606986899563319\n",
        "\n",
        "//area under the precision-recall curve: 0.8665970252411103\n",
        "//area under the (ROC) curve : 0.941260162601624"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}