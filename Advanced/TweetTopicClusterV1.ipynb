{"cells":[{"cell_type":"code","source":["1) Create term cluster for each of 4 weeks\n3) Create weekly / daily stats for each term \nhttps://docs.databricks.com/notebooks/notebooks-use.html"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansi-cyan-fg\">  File </span><span class=\"ansi-green-fg\">&#34;&lt;command-807861565787517&gt;&#34;</span><span class=\"ansi-cyan-fg\">, line </span><span class=\"ansi-green-fg\">1</span>\n<span class=\"ansi-red-fg\">    1) Create term cluster for each of 4 weeks</span>\n     ^\n<span class=\"ansi-red-fg\">SyntaxError</span><span class=\"ansi-red-fg\">:</span> invalid syntax\n</div>"]}}],"execution_count":1},{"cell_type":"code","source":["import sparknlp\n\nprint(\"Spark NLP version\")\nprint(sparknlp.version())\nprint(\"Apache Spark version\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Spark NLP version\n2.5.0\nApache Spark version\n</div>"]}}],"execution_count":2},{"cell_type":"code","source":["!pip install wordcloud"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":3},{"cell_type":"code","source":["from sparknlp.pretrained import PretrainedPipeline\nfrom pyspark.sql.functions import from_unixtime, to_date, asc, year, udf, explode, split, col, desc, length, rank, dense_rank, avg, sum\nfrom pyspark.sql.window import Window\nfrom pyspark.ml.linalg import Vectors\nfrom pyspark.ml.feature import VectorAssembler, StandardScaler\nfrom pyspark.ml.stat import Correlation\nfrom pyspark.ml.clustering import BisectingKMeans\nfrom pyspark.ml.evaluation import ClusteringEvaluator\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.tuning import CrossValidator, ParamGridBuilder\nfrom pyspark.sql.functions import col, to_timestamp,date_format\nfrom collections import Counter\nfrom wordcloud import WordCloud\n\n#from textblob import TextBlob"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":4},{"cell_type":"code","source":["df = spark.sql(\"\"\"select event_date, full_text as text from march_covid_tweets\"\"\")\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">Py4JJavaError</span>                             Traceback (most recent call last)\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     62</span>         <span class=\"ansi-green-fg\">try</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">---&gt; 63</span><span class=\"ansi-red-fg\">             </span><span class=\"ansi-green-fg\">return</span> f<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     64</span>         <span class=\"ansi-green-fg\">except</span> py4j<span class=\"ansi-blue-fg\">.</span>protocol<span class=\"ansi-blue-fg\">.</span>Py4JJavaError <span class=\"ansi-green-fg\">as</span> e<span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py</span> in <span class=\"ansi-cyan-fg\">get_return_value</span><span class=\"ansi-blue-fg\">(answer, gateway_client, target_id, name)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    327</span>                     <span class=\"ansi-blue-fg\">&#34;An error occurred while calling {0}{1}{2}.\\n&#34;</span><span class=\"ansi-blue-fg\">.</span>\n<span class=\"ansi-green-fg\">--&gt; 328</span><span class=\"ansi-red-fg\">                     format(target_id, &#34;.&#34;, name), value)\n</span><span class=\"ansi-green-intense-fg ansi-bold\">    329</span>             <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-red-fg\">Py4JJavaError</span>: An error occurred while calling o206.sql.\n: org.apache.spark.sql.AnalysisException: Table or view not found: march_covid_tweets; line 1 pos 42\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:47)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupTableFromCatalog(Analyzer.scala:749)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.resolveRelation(Analyzer.scala:694)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$8.applyOrElse(Analyzer.scala:730)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$8.applyOrElse(Analyzer.scala:723)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$apply$1.apply(AnalysisHelper.scala:90)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$apply$1.apply(AnalysisHelper.scala:90)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:76)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:89)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:194)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.resolveOperatorsUp(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$1.apply(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$1.apply(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$8.apply(TreeNode.scala:353)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:207)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:351)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:194)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.resolveOperatorsUp(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:723)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:663)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:112)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:109)\n\tat scala.collection.LinearSeqOptimized$class.foldLeft(LinearSeqOptimized.scala:124)\n\tat scala.collection.immutable.List.foldLeft(List.scala:84)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:109)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:101)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:101)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:137)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:131)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:103)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$executeAndTrack$1.apply(RuleExecutor.scala:80)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$executeAndTrack$1.apply(RuleExecutor.scala:80)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:88)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:79)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:115)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:114)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:114)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$analyzed$1.apply(QueryExecution.scala:85)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$analyzed$1.apply(QueryExecution.scala:86)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$executePhase$1.apply(QueryExecution.scala:229)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:835)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:228)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:83)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:83)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:75)\n\tat org.apache.spark.sql.Dataset$$anonfun$ofRows$2.apply(Dataset.scala:90)\n\tat org.apache.spark.sql.Dataset$$anonfun$ofRows$2.apply(Dataset.scala:88)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:835)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:88)\n\tat org.apache.spark.sql.SparkSession$$anonfun$sql$1.apply(SparkSession.scala:693)\n\tat org.apache.spark.sql.SparkSession$$anonfun$sql$1.apply(SparkSession.scala:688)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:835)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:688)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.sql.catalyst.analysis.NoSuchTableException: Table or view &#39;march_covid_tweets&#39; not found in database &#39;default&#39;;\n\tat org.apache.spark.sql.hive.client.HiveClient$$anonfun$getTable$1.apply(HiveClient.scala:81)\n\tat org.apache.spark.sql.hive.client.HiveClient$$anonfun$getTable$1.apply(HiveClient.scala:81)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.sql.hive.client.HiveClient$class.getTable(HiveClient.scala:81)\n\tat org.apache.spark.sql.hive.client.PoolingHiveClient.getTable(PoolingHiveClient.scala:42)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$getRawTable$1.apply(HiveExternalCatalog.scala:179)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$getRawTable$1.apply(HiveExternalCatalog.scala:179)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$withClient$1$$anonfun$apply$1.apply(HiveExternalCatalog.scala:144)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.org$apache$spark$sql$hive$HiveExternalCatalog$$maybeSynchronized(HiveExternalCatalog.scala:105)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$withClient$1.apply(HiveExternalCatalog.scala:142)\n\tat com.databricks.backend.daemon.driver.ProgressReporter$.withStatusCode(ProgressReporter.scala:372)\n\tat com.databricks.backend.daemon.driver.ProgressReporter$.withStatusCode(ProgressReporter.scala:358)\n\tat com.databricks.spark.util.SparkDatabricksProgressReporter$.withStatusCode(ProgressReporter.scala:34)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:140)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.getRawTable(HiveExternalCatalog.scala:178)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$getTable$1.apply(HiveExternalCatalog.scala:767)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$getTable$1.apply(HiveExternalCatalog.scala:767)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$withClient$1$$anonfun$apply$1.apply(HiveExternalCatalog.scala:144)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.org$apache$spark$sql$hive$HiveExternalCatalog$$maybeSynchronized(HiveExternalCatalog.scala:105)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$withClient$1.apply(HiveExternalCatalog.scala:142)\n\tat com.databricks.backend.daemon.driver.ProgressReporter$.withStatusCode(ProgressReporter.scala:372)\n\tat com.databricks.backend.daemon.driver.ProgressReporter$.withStatusCode(ProgressReporter.scala:358)\n\tat com.databricks.spark.util.SparkDatabricksProgressReporter$.withStatusCode(ProgressReporter.scala:34)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:140)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.getTable(HiveExternalCatalog.scala:766)\n\tat org.apache.spark.sql.catalyst.catalog.ExternalCatalogWithListener.getTable(ExternalCatalogWithListener.scala:143)\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.lookupRelation(SessionCatalog.scala:736)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupTableFromCatalog$1.apply(Analyzer.scala:746)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupTableFromCatalog$1.apply(Analyzer.scala:746)\n\tat scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:79)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupTableFromCatalog(Analyzer.scala:745)\n\t... 70 more\n\n\nDuring handling of the above exception, another exception occurred:\n\n<span class=\"ansi-red-fg\">AnalysisException</span>                         Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-2486079054882770&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-fg\">----&gt; 1</span><span class=\"ansi-red-fg\"> </span>df <span class=\"ansi-blue-fg\">=</span> spark<span class=\"ansi-blue-fg\">.</span>sql<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;&#34;&#34;select event_date, full_text as text from march_covid_tweets&#34;&#34;&#34;</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/session.py</span> in <span class=\"ansi-cyan-fg\">sql</span><span class=\"ansi-blue-fg\">(self, sqlQuery)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    834</span>         <span class=\"ansi-blue-fg\">[</span>Row<span class=\"ansi-blue-fg\">(</span>f1<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">,</span> f2<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-blue-fg\">u&#39;row1&#39;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">,</span> Row<span class=\"ansi-blue-fg\">(</span>f1<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-cyan-fg\">2</span><span class=\"ansi-blue-fg\">,</span> f2<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-blue-fg\">u&#39;row2&#39;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">,</span> Row<span class=\"ansi-blue-fg\">(</span>f1<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-cyan-fg\">3</span><span class=\"ansi-blue-fg\">,</span> f2<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-blue-fg\">u&#39;row3&#39;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">]</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    835</span>         &#34;&#34;&#34;\n<span class=\"ansi-green-fg\">--&gt; 836</span><span class=\"ansi-red-fg\">         </span><span class=\"ansi-green-fg\">return</span> DataFrame<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">.</span>_jsparkSession<span class=\"ansi-blue-fg\">.</span>sql<span class=\"ansi-blue-fg\">(</span>sqlQuery<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">,</span> self<span class=\"ansi-blue-fg\">.</span>_wrapped<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    837</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">    838</span>     <span class=\"ansi-blue-fg\">@</span>since<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-cyan-fg\">2.0</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py</span> in <span class=\"ansi-cyan-fg\">__call__</span><span class=\"ansi-blue-fg\">(self, *args)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1255</span>         answer <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>gateway_client<span class=\"ansi-blue-fg\">.</span>send_command<span class=\"ansi-blue-fg\">(</span>command<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1256</span>         return_value = get_return_value(\n<span class=\"ansi-green-fg\">-&gt; 1257</span><span class=\"ansi-red-fg\">             answer, self.gateway_client, self.target_id, self.name)\n</span><span class=\"ansi-green-intense-fg ansi-bold\">   1258</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   1259</span>         <span class=\"ansi-green-fg\">for</span> temp_arg <span class=\"ansi-green-fg\">in</span> temp_args<span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     67</span>                                              e.java_exception.getStackTrace()))\n<span class=\"ansi-green-intense-fg ansi-bold\">     68</span>             <span class=\"ansi-green-fg\">if</span> s<span class=\"ansi-blue-fg\">.</span>startswith<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;org.apache.spark.sql.AnalysisException: &#39;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">---&gt; 69</span><span class=\"ansi-red-fg\">                 </span><span class=\"ansi-green-fg\">raise</span> AnalysisException<span class=\"ansi-blue-fg\">(</span>s<span class=\"ansi-blue-fg\">.</span>split<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;: &#39;</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> stackTrace<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     70</span>             <span class=\"ansi-green-fg\">if</span> s<span class=\"ansi-blue-fg\">.</span>startswith<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;org.apache.spark.sql.catalyst.analysis&#39;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     71</span>                 <span class=\"ansi-green-fg\">raise</span> AnalysisException<span class=\"ansi-blue-fg\">(</span>s<span class=\"ansi-blue-fg\">.</span>split<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;: &#39;</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> stackTrace<span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-red-fg\">AnalysisException</span>: &#39;Table or view not found: march_covid_tweets; line 1 pos 42&#39;</div>"]}}],"execution_count":5},{"cell_type":"code","source":["data = df.withColumn(\"week\", date_format(col(\"event_date\"), \"W\"))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":6},{"cell_type":"code","source":["data.orderBy(asc(\"event_date\")).select(\"text\").show(2)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+--------------------+\n                text|\n+--------------------+\nRT @CDCgov: While...|\nRT @CDCgov: The f...|\n+--------------------+\nonly showing top 2 rows\n\n</div>"]}}],"execution_count":7},{"cell_type":"code","source":["documentExplainerPipeline = PretrainedPipeline('explain_document_dl', 'en')"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">explain_document_dl download started this may take some time.\nApprox size to download 168.4 MB\n\r[ | ]\r[OK!]\n</div>"]}}],"execution_count":8},{"cell_type":"code","source":["week4DF = data.filter((data['week'] == 4)).select(\"text\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":9},{"cell_type":"code","source":["week5DF = data.filter((data['week'] == 5)).select(\"text\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":10},{"cell_type":"code","source":["week4DF.show(2)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+--------------------+\n                text|\n+--------------------+\nRT @StephenMcDone...|\nRT @BillRatchet: ...|\n+--------------------+\nonly showing top 2 rows\n\n</div>"]}}],"execution_count":11},{"cell_type":"code","source":["!pip install nltk\nimport nltk\nfrom nltk.corpus import stopwords\nnltk.download('stopwords')\neng_stopwords = stopwords.words('english')\n#eng_stopwords.append('xxxx')\n#for item in eng_stopwords[:10]:\n#  print(item)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">[nltk_data] Downloading package stopwords to /root/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n</div>"]}}],"execution_count":12},{"cell_type":"code","source":["def wordcloud(corpus_sdf):\n    corpus_pdf = corpus_sdf.limit(500).toPandas()\n\n    corpus_dict = {}\n    for index, row in corpus_pdf.iterrows():\n        corpus_dict[row['text']] = row['count']\n        \n    wordcloud = WordCloud().generate_from_frequencies(corpus_dict)\n    plt.imshow(wordcloud);"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":13},{"cell_type":"markdown","source":["### Approach 1"],"metadata":{}},{"cell_type":"code","source":["def remove_stopwords(x):    \n    sw = stopwords.words(\"english\")\n    string = ''\n    for x in x.split(' '):\n        if x.lower() not in sw:\n            string += x + ' '\n        else:\n            pass\n    return string\n\nnosw = udf(remove_stopwords)\nspark.udf.register(\"nosw\", nosw)\nweek4DF = week4DF.withColumn('text_nosw',nosw('text'))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":15},{"cell_type":"code","source":["def corpus_creator(text_col):\n    corpus = text_col.rdd \\\n                    .flatMap(flat_list) \\\n                    .map(lambda x: (x, 1)) \\\n                    .reduceByKey(lambda x, y: x+y ) \\\n                    .sortBy(lambda x: x[1], ascending=False) \\\n                    .toDF() \\\n                    .withColumnRenamed('_1','text') \\\n                    .withColumnRenamed('_2','count')\n    return corpus"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":16},{"cell_type":"code","source":["def flat_list(column):\n    corpus = []\n    for row in column:\n        for w in row.split(' '):\n            corpus.append(w)\n    return corpus"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":17},{"cell_type":"code","source":["corpus_tweet=corpus_creator(week4DF.select(\"text_nosw\"))"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"code","source":["import matplotlib.pyplot as plt\nimport seaborn as sns\nfig, ax = plt.subplots()\nax = wordcloud(corpus_tweet)\nfig.suptitle('{} wordcloud'.format(\"2020-Jan-W4\"))"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"code","source":["week5DF = week5DF.withColumn('text_nosw',nosw('text'))\ncorpus_tweet_janw5=corpus_creator(week5DF.select(\"text_nosw\"))\nfig, ax = plt.subplots()\nax = wordcloud(corpus_tweet_janw5)\nfig.suptitle('{} wordcloud'.format(\"2020-Jan-W5\"))"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"markdown","source":["### Approach 2"],"metadata":{}},{"cell_type":"code","source":["import matplotlib.pyplot as plt\nimport seaborn as sns\n\ndef make_string(x):\n  string = ''\n  for x in x:\n    string += x + ' '\n  return string\n\nmake_string = udf(make_string)\nspark.udf.register(\"make_string\", make_string)   "],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":22},{"cell_type":"code","source":["def resolveEntities(input_corpus, week_list):\n  weekly_entities = {}\n  print(week_list)\n  input_corpus.show(5)\n  \n  for weeknum in week_list:\n      print(\"starting entity resolution\")\n      entities_filtered = documentExplainerPipeline.transform(input_corpus) \\\n                                  .select('text','count',\n                                          col('entities.result').alias('entities'),\n                                          col('pos.result').alias('pos'))\n\n      entities_filtered.show(2)\n\n      entities_filtered = entities_filtered.withColumn('entities',make_string('entities'))\\\n                                          .withColumn('pos',make_string('pos'))\\\n                                          .filter('entities <> \"\"')\n\n      entities_filtered.show(2)\n      weekly_entities[str(weeknum)] = entities_filtered\n  return weekly_entities "],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":23},{"cell_type":"code","source":["week_list = [4]\njan_weekly_entities = resolveEntities(corpus_tweet,week_list)"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"code","source":["for key,value in jan_weekly_entities.items():\n    fig, ax = plt.subplots()\n    ax = wordcloud(value)\n    fig.suptitle('{} wordcloud'.format(key))"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"markdown","source":["## Approach 3"],"metadata":{}},{"cell_type":"code","source":["from sparknlp.base import Finisher, DocumentAssembler\nfrom sparknlp.annotator import (Tokenizer, Normalizer,LemmatizerModel, StopWordsCleaner)\nfrom pyspark.ml import Pipeline\ndocumentAssembler = DocumentAssembler() \\\n    .setInputCol('text') \\\n    .setOutputCol('document')\n\ntokenizer = Tokenizer() \\\n    .setInputCols(['document']) \\\n    .setOutputCol('token')\n\n# note normalizer defaults to changing all words to lowercase.\n# Use .setLowercase(False) to maintain input case.\nnormalizer = Normalizer() \\\n    .setInputCols(['token']) \\\n    .setOutputCol('normalized') \\\n    .setLowercase(True)\n\n# note that lemmatizer needs a dictionary. So I used the pre-trained\n# model (note that it defaults to english)\nlemmatizer = LemmatizerModel.pretrained() \\\n    .setInputCols(['normalized']) \\\n    .setOutputCol('lemma') \\\n\nstopwords_cleaner = StopWordsCleaner() \\\n    .setInputCols(['lemma']) \\\n    .setOutputCol('clean_lemma') \\\n    .setCaseSensitive(False) \\\n    .setStopWords(eng_stopwords) \n\n#eng_stopwords\n\n# finisher converts tokens to human-readable output\nfinisher = Finisher() \\\n    .setInputCols(['clean_lemma']) \\\n    .setCleanAnnotations(False)\n\npipeline = Pipeline() \\\n    .setStages([\n        documentAssembler,\n        tokenizer,\n        normalizer,\n        lemmatizer,\n        stopwords_cleaner,\n        finisher\n    ])"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">lemma_antbnc download started this may take some time.\nApproximate size to download 907.6 KB\n\r[ | ]\r[ / ]\r[ — ]\r[ \\ ]\r[ | ]\r[ / ]\r[ — ]\r[ \\ ]\r[ | ]\r[OK!]\n</div>"]}}],"execution_count":27},{"cell_type":"code","source":["week4CleanDF = pipeline.fit(week4DF).transform(week4DF) # without stop words"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":28},{"cell_type":"code","source":["week5CleanDF = pipeline.fit(week5DF).transform(week5DF)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":29},{"cell_type":"code","source":["week4CleanDFV2 = pipeline.fit(week4DF).transform(week4DF)\nweek4CleanDFV2.show(10)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n                text|           text_nosw|            document|               token|          normalized|               lemma|         clean_lemma|finished_clean_lemma|\n+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\nRT @StephenMcDone...|RT @StephenMcDone...|[[document, 0, 13...|[[token, 0, 1, RT...|[[token, 0, 1, rt...|[[token, 0, 1, rt...|[[token, 0, 1, rt...|[rt, stephenmcdon...|\nRT @BillRatchet: ...|RT @BillRatchet: ...|[[document, 0, 89...|[[token, 0, 1, RT...|[[token, 0, 1, rt...|[[token, 0, 1, rt...|[[token, 0, 1, rt...|[rt, billratchet,...|\nRT @haruharu_w_bt...|RT @haruharu_w_bt...|[[document, 0, 14...|[[token, 0, 1, RT...|[[token, 0, 1, rt...|[[token, 0, 1, rt...|[[token, 0, 1, rt...|[rt, haruharuwbts...|\nRT @haruharu_w_bt...|RT @haruharu_w_bt...|[[document, 0, 14...|[[token, 0, 1, RT...|[[token, 0, 1, rt...|[[token, 0, 1, rt...|[[token, 0, 1, rt...|[rt, haruharuwbts...|\nRT @botchedcredit...|RT @botchedcredit...|[[document, 0, 11...|[[token, 0, 1, RT...|[[token, 0, 1, rt...|[[token, 0, 1, rt...|[[token, 0, 1, rt...|[rt, botchedcredi...|\nRT @AFP: #UPDATE ...|RT @AFP: #UPDATE ...|[[document, 0, 13...|[[token, 0, 1, RT...|[[token, 0, 1, rt...|[[token, 0, 1, rt...|[[token, 0, 1, rt...|[rt, afp, update,...|\nRT @celliottabili...|RT @celliottabili...|[[document, 0, 13...|[[token, 0, 1, RT...|[[token, 0, 1, rt...|[[token, 0, 1, rt...|[[token, 0, 1, rt...|[rt, celliottabil...|\nRT @TrnThiHa14: W...|RT @TrnThiHa14: W...|[[document, 0, 71...|[[token, 0, 1, RT...|[[token, 0, 1, rt...|[[token, 0, 1, rt...|[[token, 0, 1, rt...|[rt, trnthiha, wu...|\nRT @TrnThiHa14: W...|RT @TrnThiHa14: W...|[[document, 0, 71...|[[token, 0, 1, RT...|[[token, 0, 1, rt...|[[token, 0, 1, rt...|[[token, 0, 1, rt...|[rt, trnthiha, wu...|\nRT @Miedle3: ++Br...|RT @Miedle3: ++Br...|[[document, 0, 13...|[[token, 0, 1, RT...|[[token, 0, 1, rt...|[[token, 0, 1, rt...|[[token, 0, 1, rt...|[rt, miedle, ++br...|\n+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\nonly showing top 10 rows\n\n</div>"]}}],"execution_count":30},{"cell_type":"code","source":["from pyspark.sql.functions import explode, col\nweek4TokenDF = week4CleanDF.withColumn(\"exploded_text\", explode(col(\"finished_clean_lemma\")))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">NameError</span>                                 Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-3320953040114910&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      1</span> <span class=\"ansi-green-fg\">from</span> pyspark<span class=\"ansi-blue-fg\">.</span>sql<span class=\"ansi-blue-fg\">.</span>functions <span class=\"ansi-green-fg\">import</span> explode<span class=\"ansi-blue-fg\">,</span> col\n<span class=\"ansi-green-intense-fg ansi-bold\">      2</span> week4TokenDF <span class=\"ansi-blue-fg\">=</span> week4CleanDF<span class=\"ansi-blue-fg\">.</span>withColumn<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;exploded_text&#34;</span><span class=\"ansi-blue-fg\">,</span> explode<span class=\"ansi-blue-fg\">(</span>col<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;finished_clean_lemma&#34;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg\">----&gt; 3</span><span class=\"ansi-red-fg\"> </span>week5TokenDF <span class=\"ansi-blue-fg\">=</span> week5CleanDF<span class=\"ansi-blue-fg\">.</span>withColumn<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;exploded_text&#34;</span><span class=\"ansi-blue-fg\">,</span> explode<span class=\"ansi-blue-fg\">(</span>col<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;finished_clean_lemma&#34;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-red-fg\">NameError</span>: name &#39;week5CleanDF&#39; is not defined</div>"]}}],"execution_count":31},{"cell_type":"code","source":["week5TokenDF = week5CleanDF.withColumn(\"exploded_text\", explode(col(\"finished_clean_lemma\")))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":32},{"cell_type":"code","source":["countWeek4DF = week4TokenDF.groupby('exploded_text').count()\ncountWeek4DF = countWeek4DF.withColumnRenamed(\"exploded_text\",\"text\")\n#countWeek4DF.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":33},{"cell_type":"code","source":["countWeek5DF = week5TokenDF.groupby('exploded_text').count()\ncountWeek5DF = countWeek5DF.withColumnRenamed(\"exploded_text\",\"text\")\n#countWeek4DF.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":34},{"cell_type":"code","source":["countWeek5DF.repartition(150).write.format(\"orc\").mode('overwrite').saveAsTable(\"countJanWeek5DF\")"],"metadata":{},"outputs":[],"execution_count":35},{"cell_type":"code","source":["topTermsWeek4DF = spark.sql(\"\"\"select text, count from countJanWeek4DF where count > 1000 order by count desc limit 100\"\"\")\n#%sql\n#select text, count from countJanWeek4DF where count > 1000 order by count desc limit 50"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":36},{"cell_type":"code","source":["topTermsWeek5DF = spark.sql(\"\"\"select text, count from countJanWeek5DF where count > 1000 order by count desc limit 100\"\"\")\n#topTermsDF.show(100)\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">Py4JJavaError</span>                             Traceback (most recent call last)\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     62</span>         <span class=\"ansi-green-fg\">try</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">---&gt; 63</span><span class=\"ansi-red-fg\">             </span><span class=\"ansi-green-fg\">return</span> f<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     64</span>         <span class=\"ansi-green-fg\">except</span> py4j<span class=\"ansi-blue-fg\">.</span>protocol<span class=\"ansi-blue-fg\">.</span>Py4JJavaError <span class=\"ansi-green-fg\">as</span> e<span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py</span> in <span class=\"ansi-cyan-fg\">get_return_value</span><span class=\"ansi-blue-fg\">(answer, gateway_client, target_id, name)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    327</span>                     <span class=\"ansi-blue-fg\">&#34;An error occurred while calling {0}{1}{2}.\\n&#34;</span><span class=\"ansi-blue-fg\">.</span>\n<span class=\"ansi-green-fg\">--&gt; 328</span><span class=\"ansi-red-fg\">                     format(target_id, &#34;.&#34;, name), value)\n</span><span class=\"ansi-green-intense-fg ansi-bold\">    329</span>             <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-red-fg\">Py4JJavaError</span>: An error occurred while calling o214.sql.\n: org.apache.spark.sql.AnalysisException: Table or view not found: countJanWeek5DF; line 1 pos 24\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:47)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupTableFromCatalog(Analyzer.scala:755)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.resolveRelation(Analyzer.scala:699)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$8.applyOrElse(Analyzer.scala:735)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$8.applyOrElse(Analyzer.scala:728)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$apply$1.apply(AnalysisHelper.scala:90)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$apply$1.apply(AnalysisHelper.scala:90)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:76)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:89)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:194)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.resolveOperatorsUp(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$1.apply(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$1.apply(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$8.apply(TreeNode.scala:353)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:207)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:351)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:194)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.resolveOperatorsUp(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$1.apply(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$1.apply(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$8.apply(TreeNode.scala:353)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:207)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:351)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:194)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.resolveOperatorsUp(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$1.apply(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$1.apply(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$8.apply(TreeNode.scala:353)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:207)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:351)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:194)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.resolveOperatorsUp(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$1.apply(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$1.apply(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$8.apply(TreeNode.scala:353)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:207)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:351)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:194)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.resolveOperatorsUp(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$1.apply(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$1.apply(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$8.apply(TreeNode.scala:353)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:207)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:351)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:194)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.resolveOperatorsUp(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:728)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:668)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:112)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:109)\n\tat scala.collection.LinearSeqOptimized$class.foldLeft(LinearSeqOptimized.scala:124)\n\tat scala.collection.immutable.List.foldLeft(List.scala:84)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:109)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:101)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:101)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:137)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:131)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:103)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$executeAndTrack$1.apply(RuleExecutor.scala:80)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$executeAndTrack$1.apply(RuleExecutor.scala:80)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:88)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:79)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:115)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:114)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:114)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$analyzed$1.apply(QueryExecution.scala:85)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$analyzed$1.apply(QueryExecution.scala:86)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$executePhase$1.apply(QueryExecution.scala:229)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:835)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:228)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:83)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:83)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:75)\n\tat org.apache.spark.sql.Dataset$$anonfun$ofRows$2.apply(Dataset.scala:90)\n\tat org.apache.spark.sql.Dataset$$anonfun$ofRows$2.apply(Dataset.scala:88)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:835)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:88)\n\tat org.apache.spark.sql.SparkSession$$anonfun$sql$1.apply(SparkSession.scala:693)\n\tat org.apache.spark.sql.SparkSession$$anonfun$sql$1.apply(SparkSession.scala:688)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:835)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:688)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.sql.catalyst.analysis.NoSuchTableException: Table or view &#39;countjanweek5df&#39; not found in database &#39;default&#39;;\n\tat org.apache.spark.sql.hive.client.HiveClient$$anonfun$getTable$1.apply(HiveClient.scala:81)\n\tat org.apache.spark.sql.hive.client.HiveClient$$anonfun$getTable$1.apply(HiveClient.scala:81)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.sql.hive.client.HiveClient$class.getTable(HiveClient.scala:81)\n\tat org.apache.spark.sql.hive.client.PoolingHiveClient.getTable(PoolingHiveClient.scala:42)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$getRawTable$1.apply(HiveExternalCatalog.scala:179)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$getRawTable$1.apply(HiveExternalCatalog.scala:179)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$withClient$1$$anonfun$apply$1.apply(HiveExternalCatalog.scala:144)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.org$apache$spark$sql$hive$HiveExternalCatalog$$maybeSynchronized(HiveExternalCatalog.scala:105)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$withClient$1.apply(HiveExternalCatalog.scala:142)\n\tat com.databricks.backend.daemon.driver.ProgressReporter$.withStatusCode(ProgressReporter.scala:372)\n\tat com.databricks.backend.daemon.driver.ProgressReporter$.withStatusCode(ProgressReporter.scala:358)\n\tat com.databricks.spark.util.SparkDatabricksProgressReporter$.withStatusCode(ProgressReporter.scala:34)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:140)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.getRawTable(HiveExternalCatalog.scala:178)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$getTable$1.apply(HiveExternalCatalog.scala:775)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$getTable$1.apply(HiveExternalCatalog.scala:775)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$withClient$1$$anonfun$apply$1.apply(HiveExternalCatalog.scala:144)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.org$apache$spark$sql$hive$HiveExternalCatalog$$maybeSynchronized(HiveExternalCatalog.scala:105)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$withClient$1.apply(HiveExternalCatalog.scala:142)\n\tat com.databricks.backend.daemon.driver.ProgressReporter$.withStatusCode(ProgressReporter.scala:372)\n\tat com.databricks.backend.daemon.driver.ProgressReporter$.withStatusCode(ProgressReporter.scala:358)\n\tat com.databricks.spark.util.SparkDatabricksProgressReporter$.withStatusCode(ProgressReporter.scala:34)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:140)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.getTable(HiveExternalCatalog.scala:774)\n\tat org.apache.spark.sql.catalyst.catalog.ExternalCatalogWithListener.getTable(ExternalCatalogWithListener.scala:143)\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.lookupRelation(SessionCatalog.scala:736)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupTableFromCatalog$1.apply(Analyzer.scala:752)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupTableFromCatalog$1.apply(Analyzer.scala:752)\n\tat scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:79)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupTableFromCatalog(Analyzer.scala:751)\n\t... 110 more\n\n\nDuring handling of the above exception, another exception occurred:\n\n<span class=\"ansi-red-fg\">AnalysisException</span>                         Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-2127994578989784&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-fg\">----&gt; 1</span><span class=\"ansi-red-fg\"> </span>topTermsWeek5DF <span class=\"ansi-blue-fg\">=</span> spark<span class=\"ansi-blue-fg\">.</span>sql<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;&#34;&#34;select text, count from countJanWeek5DF where count &gt; 1000 order by count desc limit 500&#34;&#34;&#34;</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      2</span> <span class=\"ansi-red-fg\">#topTermsDF.show(100)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/session.py</span> in <span class=\"ansi-cyan-fg\">sql</span><span class=\"ansi-blue-fg\">(self, sqlQuery)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    834</span>         <span class=\"ansi-blue-fg\">[</span>Row<span class=\"ansi-blue-fg\">(</span>f1<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">,</span> f2<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-blue-fg\">u&#39;row1&#39;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">,</span> Row<span class=\"ansi-blue-fg\">(</span>f1<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-cyan-fg\">2</span><span class=\"ansi-blue-fg\">,</span> f2<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-blue-fg\">u&#39;row2&#39;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">,</span> Row<span class=\"ansi-blue-fg\">(</span>f1<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-cyan-fg\">3</span><span class=\"ansi-blue-fg\">,</span> f2<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-blue-fg\">u&#39;row3&#39;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">]</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    835</span>         &#34;&#34;&#34;\n<span class=\"ansi-green-fg\">--&gt; 836</span><span class=\"ansi-red-fg\">         </span><span class=\"ansi-green-fg\">return</span> DataFrame<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">.</span>_jsparkSession<span class=\"ansi-blue-fg\">.</span>sql<span class=\"ansi-blue-fg\">(</span>sqlQuery<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">,</span> self<span class=\"ansi-blue-fg\">.</span>_wrapped<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    837</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">    838</span>     <span class=\"ansi-blue-fg\">@</span>since<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-cyan-fg\">2.0</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py</span> in <span class=\"ansi-cyan-fg\">__call__</span><span class=\"ansi-blue-fg\">(self, *args)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1255</span>         answer <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>gateway_client<span class=\"ansi-blue-fg\">.</span>send_command<span class=\"ansi-blue-fg\">(</span>command<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1256</span>         return_value = get_return_value(\n<span class=\"ansi-green-fg\">-&gt; 1257</span><span class=\"ansi-red-fg\">             answer, self.gateway_client, self.target_id, self.name)\n</span><span class=\"ansi-green-intense-fg ansi-bold\">   1258</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   1259</span>         <span class=\"ansi-green-fg\">for</span> temp_arg <span class=\"ansi-green-fg\">in</span> temp_args<span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     67</span>                                              e.java_exception.getStackTrace()))\n<span class=\"ansi-green-intense-fg ansi-bold\">     68</span>             <span class=\"ansi-green-fg\">if</span> s<span class=\"ansi-blue-fg\">.</span>startswith<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;org.apache.spark.sql.AnalysisException: &#39;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">---&gt; 69</span><span class=\"ansi-red-fg\">                 </span><span class=\"ansi-green-fg\">raise</span> AnalysisException<span class=\"ansi-blue-fg\">(</span>s<span class=\"ansi-blue-fg\">.</span>split<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;: &#39;</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> stackTrace<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     70</span>             <span class=\"ansi-green-fg\">if</span> s<span class=\"ansi-blue-fg\">.</span>startswith<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;org.apache.spark.sql.catalyst.analysis&#39;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     71</span>                 <span class=\"ansi-green-fg\">raise</span> AnalysisException<span class=\"ansi-blue-fg\">(</span>s<span class=\"ansi-blue-fg\">.</span>split<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;: &#39;</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> stackTrace<span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-red-fg\">AnalysisException</span>: &#39;Table or view not found: countJanWeek5DF; line 1 pos 24&#39;</div>"]}}],"execution_count":37},{"cell_type":"code","source":["fig, ax = plt.subplots()\nax = wordcloud(topTermsWeek4DF)\nfig.suptitle('{} wordcloud'.format(\"2020-01-W4\"))"],"metadata":{},"outputs":[],"execution_count":38},{"cell_type":"code","source":["topTermsWeek4DF.show(2)\njan_weekly_entities['4'].show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----------+-------+\n       text|  count|\n+-----------+-------+\n         rt|1342192|\ncoronavirus| 826204|\n+-----------+-------+\nonly showing top 2 rows\n\n+-----------------+-------+------------------+------+\n             text|  count|          entities|   pos|\n+-----------------+-------+------------------+------+\n               RT|1273166|               RT |  NNP |\n            Wuhan| 239418|            Wuhan |  NNP |\n            China| 223525|            China |  NNP |\n          Chinese| 107526|          Chinese |   JJ |\n      Coronavirus|  78013|      Coronavirus |   NN |\n           #Wuhan|  73936|           #Wuhan |  NNP |\n            &amp;amp;|  56330|             &amp;amp | NN : |\n             Hong|  50826|             Hong |  NNP |\n           Wuhan,|  46795|            Wuhan |NNP , |\n               US|  44092|               US |  NNP |\n#WuhanCoronavirus|  33194|#WuhanCoronavirus |   NN |\n           #China|  31872|           #China |  NNP |\n              CDC|  30479|              CDC |  NNP |\n             Kong|  29924|             Kong |  NNP |\n     #Coronavirus|  27273|     #Coronavirus |   NN |\n              New|  24505|              New |  NNP |\n           United|  24410|           United |  NNP |\n             U.S.|  23918|              U.S |NNP . |\n           States|  21810|           States |  NNS |\n       University|  20052|       University |  NNP |\n+-----------------+-------+------------------+------+\nonly showing top 20 rows\n\n</div>"]}}],"execution_count":39},{"cell_type":"code","source":["fig, ax = plt.subplots()\nax = wordcloud(topTermsWeek5DF)\nfig.suptitle('{} wordcloud'.format(\"2020-01-W5\"))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\njava.rmi.RemoteException: com.databricks.api.base.DatabricksServiceException: INTERNAL_ERROR: Unexpected error.; nested exception is: \n\tcom.databricks.api.base.DatabricksServiceException: INTERNAL_ERROR: Unexpected error.\n\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:123)\n\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIgnoreDraining(DbfsClient.scala:83)\n\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close0(DbfsOutputStream.scala:100)\n\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close(DbfsOutputStream.scala:79)\n\tat java.io.FilterOutputStream.close(FilterOutputStream.java:159)\n\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)\n\tat org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)\n\tat org.apache.hadoop.io.IOUtils.copyBytes(IOUtils.java:61)\n\tat org.apache.hadoop.io.IOUtils.copyBytes(IOUtils.java:105)\n\tat org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:443)\n\tat com.databricks.backend.daemon.driver.FileStorePlotImageSaver.savePlotImage(FileStorePlotImageSaver.scala:32)\n\tat com.databricks.backend.daemon.driver.PythonDriverLocal$$anonfun$getResultBufferInternal$1$$anonfun$11.apply(PythonDriverLocal.scala:962)\n\tat com.databricks.backend.daemon.driver.PythonDriverLocal$$anonfun$getResultBufferInternal$1$$anonfun$11.apply(PythonDriverLocal.scala:962)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1334)\n\tat scala.collection.IterableLike$class.foreach(IterableLike.scala:72)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:54)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:104)\n\tat com.databricks.backend.daemon.driver.PythonDriverLocal$$anonfun$getResultBufferInternal$1.apply(PythonDriverLocal.scala:962)\n\tat com.databricks.backend.daemon.driver.PythonDriverLocal$$anonfun$getResultBufferInternal$1.apply(PythonDriverLocal.scala:936)\n\tat com.databricks.backend.daemon.driver.PythonDriverLocal.withInterpLock(PythonDriverLocal.scala:881)\n\tat com.databricks.backend.daemon.driver.PythonDriverLocal.getResultBufferInternal(PythonDriverLocal.scala:936)\n\tat com.databricks.backend.daemon.driver.DriverLocal.getResultBuffer(DriverLocal.scala:492)\n\tat com.databricks.backend.daemon.driver.PythonDriverLocal.com$databricks$backend$daemon$driver$PythonDriverLocal$$outputSuccess(PythonDriverLocal.scala:923)\n\tat com.databricks.backend.daemon.driver.PythonDriverLocal$$anonfun$repl$6.apply(PythonDriverLocal.scala:369)\n\tat com.databricks.backend.daemon.driver.PythonDriverLocal$$anonfun$repl$6.apply(PythonDriverLocal.scala:356)\n\tat com.databricks.backend.daemon.driver.PythonDriverLocal.withInterpLock(PythonDriverLocal.scala:881)\n\tat com.databricks.backend.daemon.driver.PythonDriverLocal.repl(PythonDriverLocal.scala:356)\n\tat com.databricks.backend.daemon.driver.DriverLocal$$anonfun$execute$9.apply(DriverLocal.scala:396)\n\tat com.databricks.backend.daemon.driver.DriverLocal$$anonfun$execute$9.apply(DriverLocal.scala:373)\n\tat com.databricks.logging.UsageLogging$$anonfun$withAttributionContext$1.apply(UsageLogging.scala:238)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)\n\tat com.databricks.logging.UsageLogging$class.withAttributionContext(UsageLogging.scala:233)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:49)\n\tat com.databricks.logging.UsageLogging$class.withAttributionTags(UsageLogging.scala:275)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:49)\n\tat com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:373)\n\tat com.databricks.backend.daemon.driver.DriverWrapper$$anonfun$tryExecutingCommand$2.apply(DriverWrapper.scala:645)\n\tat com.databricks.backend.daemon.driver.DriverWrapper$$anonfun$tryExecutingCommand$2.apply(DriverWrapper.scala:645)\n\tat scala.util.Try$.apply(Try.scala:192)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:640)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.getCommandOutputAndError(DriverWrapper.scala:486)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:598)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:391)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:337)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:219)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: com.databricks.api.base.DatabricksServiceException: INTERNAL_ERROR: Unexpected error.\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException(ScalaProto2RpcSerializer.scala:315)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException$(ScalaProto2RpcSerializer.scala:309)\n\tat com.databricks.api.rpc.ScalaProto2CodeGeneratedRpcSerializer.deserializeException(ScalaProto2CodeGeneratedRpcSerializer.scala:34)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException(ScalaProto2RpcSerializer.scala:306)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException$(ScalaProto2RpcSerializer.scala:305)\n\tat com.databricks.api.rpc.ScalaProto2CodeGeneratedRpcSerializer.deserializeException(ScalaProto2CodeGeneratedRpcSerializer.scala:34)\n\tat com.databricks.rpc.Jetty9Client$$anon$1.$anonfun$onComplete$4(Jetty9Client.scala:541)\n\tat com.databricks.util.UntrustedUtils$.logUncaughtExceptions(UntrustedUtils.scala:34)\n\tat com.databricks.rpc.Jetty9Client$$anon$1.onComplete(Jetty9Client.scala:539)\n\tat shaded.v9_4.org.eclipse.jetty.client.ResponseNotifier.notifyComplete(ResponseNotifier.java:198)\n\tat shaded.v9_4.org.eclipse.jetty.client.ResponseNotifier.notifyComplete(ResponseNotifier.java:190)\n\tat shaded.v9_4.org.eclipse.jetty.client.HttpReceiver.terminateResponse(HttpReceiver.java:444)\n\tat shaded.v9_4.org.eclipse.jetty.client.HttpReceiver.responseSuccess(HttpReceiver.java:390)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.messageComplete(HttpReceiverOverHTTP.java:316)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.handleContentMessage(HttpParser.java:574)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.parseContent(HttpParser.java:1644)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:1490)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.parse(HttpReceiverOverHTTP.java:172)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.process(HttpReceiverOverHTTP.java:135)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.receive(HttpReceiverOverHTTP.java:73)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpChannelOverHTTP.receive(HttpChannelOverHTTP.java:133)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpConnectionOverHTTP.onFillable(HttpConnectionOverHTTP.java:151)\n\tat shaded.v9_4.org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n\tat shaded.v9_4.org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection$DecryptedEndPoint.onFillable(SslConnection.java:426)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection.onFillable(SslConnection.java:320)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection$2.succeeded(SslConnection.java:158)\n\tat shaded.v9_4.org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)\n\tat shaded.v9_4.org.eclipse.jetty.io.ChannelEndPoint$2.run(ChannelEndPoint.java:117)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:336)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:313)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:129)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:367)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:782)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:914)\n\t... 1 more\n\tSuppressed: com.databricks.rpc.Jetty9Client$Jetty9ClientException: Exception in send()\n\t\tat com.databricks.rpc.Jetty9Client.send(Jetty9Client.scala:145)\n\t\tat com.databricks.rpc.DynamicJettyClient.send(BaseJettyClient.scala:548)\n\t\tat com.databricks.rpc.BoundRPCClient.send(BoundRPCClient.scala:36)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient$$anonfun$doSend$2.apply(DbfsClient.scala:161)\n\t\tat com.databricks.logging.UsageLogging$$anonfun$withAttributionContext$1.apply(UsageLogging.scala:238)\n\t\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)\n\t\tat com.databricks.logging.UsageLogging$class.withAttributionContext(UsageLogging.scala:233)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.withAttributionContext(DbfsClient.scala:23)\n\t\tat com.databricks.logging.UsageLogging$class.withAttributionTags(UsageLogging.scala:275)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.withAttributionTags(DbfsClient.scala:23)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.doSend(DbfsClient.scala:147)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:98)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIgnoreDraining(DbfsClient.scala:83)\n\t\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close0(DbfsOutputStream.scala:100)\n\t\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close(DbfsOutputStream.scala:79)\n\t\tat java.io.FilterOutputStream.close(FilterOutputStream.java:159)\n\t\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)\n\t\tat org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)\n\t\tat org.apache.hadoop.io.IOUtils.copyBytes(IOUtils.java:61)\n\t\tat org.apache.hadoop.io.IOUtils.copyBytes(IOUtils.java:105)\n\t\tat org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:443)\n\t\tat com.databricks.backend.daemon.driver.FileStorePlotImageSaver.savePlotImage(FileStorePlotImageSaver.scala:32)\n\t\tat com.databricks.backend.daemon.driver.PythonDriverLocal$$anonfun$getResultBufferInternal$1$$anonfun$11.apply(PythonDriverLocal.scala:962)\n\t\tat com.databricks.backend.daemon.driver.PythonDriverLocal$$anonfun$getResultBufferInternal$1$$anonfun$11.apply(PythonDriverLocal.scala:962)\n\t\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\t\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\t\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\t\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1334)\n\t\tat scala.collection.IterableLike$class.foreach(IterableLike.scala:72)\n\t\tat scala.collection.AbstractIterable.foreach(Iterable.scala:54)\n\t\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\t\tat scala.collection.AbstractTraversable.map(Traversable.scala:104)\n\t\tat com.databricks.backend.daemon.driver.PythonDriverLocal$$anonfun$getResultBufferInternal$1.apply(PythonDriverLocal.scala:962)\n\t\tat com.databricks.backend.daemon.driver.PythonDriverLocal$$anonfun$getResultBufferInternal$1.apply(PythonDriverLocal.scala:936)\n\t\tat com.databricks.backend.daemon.driver.PythonDriverLocal.withInterpLock(PythonDriverLocal.scala:881)\n\t\tat com.databricks.backend.daemon.driver.PythonDriverLocal.getResultBufferInternal(PythonDriverLocal.scala:936)\n\t\tat com.databricks.backend.daemon.driver.DriverLocal.getResultBuffer(DriverLocal.scala:492)\n\t\tat com.databricks.backend.daemon.driver.PythonDriverLocal.com$databricks$backend$daemon$driver$PythonDriverLocal$$outputSuccess(PythonDriverLocal.scala:923)\n\t\tat com.databricks.backend.daemon.driver.PythonDriverLocal$$anonfun$repl$6.apply(PythonDriverLocal.scala:369)\n\t\tat com.databricks.backend.daemon.driver.PythonDriverLocal$$anonfun$repl$6.apply(PythonDriverLocal.scala:356)\n\t\tat com.databricks.backend.daemon.driver.PythonDriverLocal.withInterpLock(PythonDriverLocal.scala:881)\n\t\tat com.databricks.backend.daemon.driver.PythonDriverLocal.repl(PythonDriverLocal.scala:356)\n\t\tat com.databricks.backend.daemon.driver.DriverLocal$$anonfun$execute$9.apply(DriverLocal.scala:396)\n\t\tat com.databricks.backend.daemon.driver.DriverLocal$$anonfun$execute$9.apply(DriverLocal.scala:373)\n\t\tat com.databricks.logging.UsageLogging$$anonfun$withAttributionContext$1.apply(UsageLogging.scala:238)\n\t\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)\n\t\tat com.databricks.logging.UsageLogging$class.withAttributionContext(UsageLogging.scala:233)\n\t\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:49)\n\t\tat com.databricks.logging.UsageLogging$class.withAttributionTags(UsageLogging.scala:275)\n\t\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:49)\n\t\tat com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:373)\n\t\tat com.databricks.backend.daemon.driver.DriverWrapper$$anonfun$tryExecutingCommand$2.apply(DriverWrapper.scala:645)\n\t\tat com.databricks.backend.daemon.driver.DriverWrapper$$anonfun$tryExecutingCommand$2.apply(DriverWrapper.scala:645)\n\t\tat scala.util.Try$.apply(Try.scala:192)\n\t\tat com.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:640)\n\t\tat com.databricks.backend.daemon.driver.DriverWrapper.getCommandOutputAndError(DriverWrapper.scala:486)\n\t\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:598)\n\t\tat com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:391)\n\t\tat com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:337)\n\t\tat com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:219)\n\t\t... 1 more"]}}],"execution_count":40},{"cell_type":"markdown","source":["##Approach 4"],"metadata":{}},{"cell_type":"code","source":["week_list = [4]\njan_weekly_entities_v2 = resolveEntities(topTermsWeek4DF,week_list)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">[4]\n+-----------+-------+\n       text|  count|\n+-----------+-------+\n         rt|1342192|\ncoronavirus| 826204|\n      wuhan| 467435|\n      china| 343293|\n       case| 186141|\n+-----------+-------+\nonly showing top 5 rows\n\nstarting entity resolution\n+-----------+-------+--------+-----+\n       text|  count|entities|  pos|\n+-----------+-------+--------+-----+\n         rt|1342192|      []|[NNP]|\ncoronavirus| 826204|      []| [NN]|\n+-----------+-------+--------+-----+\nonly showing top 2 rows\n\n+----+-----+--------+---+\ntext|count|entities|pos|\n+----+-----+--------+---+\n ive| 8376|    ive |JJ |\n+----+-----+--------+---+\n\n</div>"]}}],"execution_count":42},{"cell_type":"code","source":["\nfor key,value in jan_weekly_entities_v2.items():\n    fig, ax = plt.subplots()\n    ax = wordcloud(value)\n    fig.suptitle('{} wordcloud'.format(key))"],"metadata":{},"outputs":[],"execution_count":43},{"cell_type":"code","source":["topTermsWeek4DF.show(20)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----------+-------+\n       text|  count|\n+-----------+-------+\n         rt|1342192|\ncoronavirus| 826204|\n      wuhan| 467435|\n      china| 343293|\n       case| 186141|\n   outbreak| 181430|\n      virus| 168359|\n        new| 161240|\n     people| 131099|\n    confirm| 128874|\n    chinese| 122035|\n   hospital| 115500|\n     health|  95157|\n     spread|  93908|\n       city|  82392|\n      break|  81646|\n        say|  76658|\n    patient|  74939|\n       life|  73853|\n      first|  71553|\n+-----------+-------+\nonly showing top 20 rows\n\n</div>"]}}],"execution_count":44},{"cell_type":"code","source":["from sparknlp.pretrained import PretrainedPipeline\npipeline1 = PretrainedPipeline('recognize_entities_dl', 'en')\n\npipeline2 = PretrainedPipeline(\"onto_recognize_entities_sm\", \"en\")\n\npredictions1 = pipeline1.transform(data)\npredictions1EntityResult=predictions1.select(\"entities.result\")\npredictions1EntityResult.show(10)\n#predictions1EntityResult.printSchema()\n\npredictions1NerResult=predictions1.select(\"ner.result\")\npredictions1NerResult.show(1)\npredictions1NerResult.printSchema()\n\npredictions2 = pipeline2.transform(data)\npredictions2.select(\"entities.result\").show(5)\npredictions2.select(\"ner.result\").show(2)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+--------------------+\n              result|\n+--------------------+\n[B-ORG, I-ORG, O,...|\n+--------------------+\nonly showing top 1 row\n\nroot\n-- result: array (nullable = true)\n    |-- element: string (containsNull = true)\n\n</div>"]}}],"execution_count":45}],"metadata":{"name":"TweetTopicClusterV1","notebookId":2486079054882765},"nbformat":4,"nbformat_minor":0}
