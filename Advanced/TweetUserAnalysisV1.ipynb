{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sparknlp\n",
    "import json\n",
    "import os\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql import SparkSession\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp.base import *\n",
    "from pyspark.sql.functions import from_unixtime\n",
    "from pyspark.sql.functions import unix_timestamp\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.ml.stat import Correlation\n",
    "from pyspark.ml.clustering import BisectingKMeans\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.sql.functions import col, to_timestamp,date_format\n",
    "from pyspark import StorageLevel\n",
    "import pyspark.sql.functions as F\n",
    "from sparknlp.pretrained import PretrainedPipeline\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "from sparknlp.base import Finisher, DocumentAssembler\n",
    "from sparknlp.annotator import (Tokenizer, Normalizer,LemmatizerModel, StopWordsCleaner)\n",
    "from pyspark.ml import Pipeline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def start():\n",
    "    builder = SparkSession.builder \\\n",
    "        .appName(\"Spark NLP Licensed\") \\\n",
    "        .master(\"local[*]\") \\\n",
    "        .config(\"spark.driver.memory\", \"24G\") \\\n",
    "        .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "        .config(\"spark.kryoserializer.buffer.max\", \"2040M\") \\\n",
    "        .config(\"spark.jars.packages\", \"com.johnsnowlabs.nlp:spark-nlp_2.11:2.5.1\") \\\n",
    "        .config(\"fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\") \\\n",
    "        .config(\"fs.AbstractFileSystem.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\")\n",
    "    return builder.getOrCreate()\n",
    "spark = start()\n",
    "spark.version\n",
    "!ps -ef | grep spark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.version\n",
    "!ps -ef | grep spark\n",
    "!free -g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sourceData1 = spark.read.format(\"json\").load(\"gs://covid19-tweets/2020-04/coronavirus-tweet-id-2020-04-15*.jsonl.gz\")\n",
    "#sourceData1.repartition(100).write.save(\"parquetFile1.parquet\")\n",
    "parquetFile1 = spark.read.parquet(\"parquetFile1.parquet\")\n",
    "parquetFile1.createOrReplaceTempView(\"tweetView\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sourceData2 = spark.read.format(\"json\").load(\"gs://covid19-tweets/2020-04/*.gz\")\n",
    "sourceData2.repartition(200).write.save(\"parquetFile2.parquet\")\n",
    "parquetFile2 = spark.read.parquet(\"parquetFile2.parquet\")\n",
    "parquetFile2.createOrReplaceTempView(\"tweetView2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tempDF1 = spark.sql(\"\"\"\n",
    "SELECT entities.hashtags.text AS hashtags, COUNT(*) as cnt\n",
    "FROM tweetView\n",
    "GROUP BY hashtags\n",
    "ORDER BY cnt DESC limit 50\n",
    "\"\"\")\n",
    "tempDF1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+\n",
      "|            hashtags|   cnt|\n",
      "+--------------------+------+\n",
      "|                  []|720335|\n",
      "|           [COVID19]| 26611|\n",
      "|       [coronavirus]|  9560|\n",
      "|           [Covid19]|  3742|\n",
      "|       [Coronavirus]|  3348|\n",
      "|             [China]|  2379|\n",
      "|[BaksosTniPolriut...|  1998|\n",
      "|           [covid19]|  1596|\n",
      "|          [StayHome]|  1566|\n",
      "|             [COVID]|  1510|\n",
      "| [DapurUmumTNIPolri]|  1190|\n",
      "|[SuperM, Together...|  1185|\n",
      "|          [BREAKING]|  1049|\n",
      "|          [lockdown]|   940|\n",
      "|         [COVID„Éº19]|   902|\n",
      "|[COVID19, coronav...|   701|\n",
      "|            [Corona]|   658|\n",
      "|          [Covid_19]|   622|\n",
      "|         [‡πÄ‡∏Ç‡∏∑‡πà‡∏≠‡∏ô‡πÇ‡∏Ç‡∏á]|   609|\n",
      "|        [StayAtHome]|   607|\n",
      "+--------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tempDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tempDF2 = spark.sql('''\n",
    "SELECT entities.user_mentions.name AS mentioned_user, COUNT(*) as cnt\n",
    "FROM tweetView\n",
    "GROUP BY mentioned_user\n",
    "ORDER BY cnt DESC limit 20\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tempDF2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---------------+-------------+--------------+---------------+\n",
      "|username           |sensitive_count|retweet_count|favorite_count|followers_count|\n",
      "+-------------------+---------------+-------------+--------------+---------------+\n",
      "|Richard gibb       |1              |108          |0             |2922           |\n",
      "|Lean Cartel Dreular|1              |0            |0             |15             |\n",
      "|Toni Tannoury üá±üáß |1              |5            |9             |860            |\n",
      "|Nnamdi UK          |1              |70           |0             |712            |\n",
      "|FXL                |1              |194          |0             |347            |\n",
      "|FEMBEAUTIES        |1              |6            |32            |13077          |\n",
      "|REBEL CITY RECORDS |1              |16           |65            |365            |\n",
      "|freedom            |1              |2543         |0             |16             |\n",
      "|Occupy Schagen     |1              |6            |0             |8955           |\n",
      "|Disgruntled        |1              |142          |0             |1719           |\n",
      "+-------------------+---------------+-------------+--------------+---------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3  = spark.sql('''SELECT user.name as username,   \n",
    " CASE WHEN possibly_sensitive = true THEN 1 ELSE 0 END AS sensitive_count,\n",
    " retweet_count , favorite_count, user.followers_count as followers_count\n",
    "from tweetView where possibly_sensitive = true and lang = 'en' and lower(concat_ws(' ',entities.hashtags.text)) rlike 'corona|covid|stay|wuhan|virus|who|cdc|trump|epidemic|pandemic|outbreak' ''')\n",
    "\n",
    "df3.show(10,False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------+-------------+---------------+---------------+\n",
      "|            username|sensitive_count|retweet_count|favourite_count|followers_count|\n",
      "+--------------------+---------------+-------------+---------------+---------------+\n",
      "|         V. D. ‡§®‡§ø‡§ñ‡§ø‡§≤|              3|            0|              0|             12|\n",
      "|            Cheer-Up|              3|            1|              4|           8186|\n",
      "|       Kenlam274üá≠üá∞|              2|            1|              3|            471|\n",
      "|ü¶†Ô∏èüó£ERADICATE CO...|              2|            0|              0|           1711|\n",
      "|Erotic Art Photog...|              2|            1|              3|            398|\n",
      "|        Tammy Searle|              2|            0|              0|           3651|\n",
      "|           Nico Gaia|              2|            8|              8|          35473|\n",
      "|          Sabah Alam|              2|            0|              2|           3285|\n",
      "|  SNAP : FREAKYGUY_X|              2|            0|              0|             26|\n",
      "|         FARED_ALHOR|              2|            8|              6|           3753|\n",
      "|         Scott Lucas|              2|            1|              5|           9464|\n",
      "|        Dianna Fresh|              2|            0|              0|             31|\n",
      "|      Charley Chetto|              2|            0|              0|            125|\n",
      "|     Donald Mccarthy|              2|            2|              2|            258|\n",
      "|                nala|              2|            1|              4|          52413|\n",
      "|          Matthew_US|              2|            0|              0|             16|\n",
      "|       JimJamesJones|              2|           16|              0|            710|\n",
      "|       DaSe‚ö°Ô∏èProject|              2|            0|              0|            160|\n",
      "|             SPINGLE|              2|            0|              0|              0|\n",
      "|Joe Biden fan 4 l...|              2|            0|              0|            345|\n",
      "+--------------------+---------------+-------------+---------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3.createOrReplaceTempView(\"selective_sensitive_tweets\")\n",
    "                 \n",
    "df6= spark.sql('''SELECT username, \n",
    "sum(sensitive_count) as sensitive_count, (sum(retweet_count)) as retweet_count, \n",
    "(sum(favorite_count)) as favourite_count, first(followers_count) as followers_count \n",
    "from selective_sensitive_tweets group by username order by sensitive_count desc limit 50''')\n",
    "\n",
    "df6.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------+-------------+---------------+---------------+\n",
      "|            username|sensitive_count|retweet_count|favourite_count|followers_count|\n",
      "+--------------------+---------------+-------------+---------------+---------------+\n",
      "| Somsirsa Chatterjee|             19|            0|              0|            796|\n",
      "|   Against Ignorance|             10|            0|              0|            157|\n",
      "|Prince Neal_Agniv...|              9|            0|              0|            318|\n",
      "|‡∏î‡∏Å‡∏ô‡∏õ‡∏´‡∏îüáªüá≥üá∫üá∏üáØ?...|              8|            9|              0|           2775|\n",
      "|Francesca BaiMuDa...|              8|           12|              0|           4926|\n",
      "|              George|              8|            0|              0|            434|\n",
      "|      Kim Kardashian|              8|            0|              0|           1030|\n",
      "|      uMbhali Wodumo|              7|            0|              0|           3645|\n",
      "|            James Wu|              7|            7|              7|              2|\n",
      "| The Daily Lafayette|              7|            0|              2|            693|\n",
      "|     Birmingham Live|              7|           11|             28|         297097|\n",
      "|         Miles to Go|              7|           76|              2|            816|\n",
      "|            King Lee|              6|            0|              0|              7|\n",
      "|Servelan, reclaim...|              6|            1|              0|           2584|\n",
      "|Sir Gary The Econ...|              6|          120|              0|           1391|\n",
      "|   CATHERINE STEVENS|              6|            0|              0|             67|\n",
      "|      CafeNetAmerica|              5|            1|              0|           5551|\n",
      "|Nectes Gospel Med...|              5|            0|              0|            284|\n",
      "|    Sioux Falls News|              5|            0|              0|             90|\n",
      "|       News365.co.za|              5|            0|              0|          11904|\n",
      "+--------------------+---------------+-------------+---------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#df3.createOrReplaceTempView(\"sensitive_tweets\")\n",
    "                 \n",
    "df4= spark.sql('''SELECT username, \n",
    "sum(sensitive_count) as sensitive_count, (sum(retweet_count)) as retweet_count, \n",
    "(sum(favorite_count)) as favourite_count, first(followers_count) as followers_count \n",
    "from sensitive_tweets group by username order by sensitive_count desc limit 50''')\n",
    "\n",
    "df4.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4= spark.sql('''SELECT username, \n",
    "sum(sensitive_count) as sensitive_count, (sum(retweet_count)) as retweet_count, \n",
    "(sum(favorite_count)) as favourite_count, first(followers_count) as followers_count \n",
    "from sensitive_tweets group by username order by sensitive_count asc limit 50''')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------+-------------+---------------+---------------+\n",
      "|            username|sensitive_count|retweet_count|favourite_count|followers_count|\n",
      "+--------------------+---------------+-------------+---------------+---------------+\n",
      "|             cubbyüí´|              1|         5399|              0|              9|\n",
      "|         Covfefe4EVA|              1|           48|              0|           1940|\n",
      "|        Poorvi Sapra|              1|            1|             12|             19|\n",
      "|          ‡∏Ñ‡∏∏‡∏ì‡∏ô‡∏≤‡∏¢‡∏û‡∏•‡∏≠‡∏¢|              1|         3388|              0|              5|\n",
      "|                  ü•µ|              1|          542|              0|           3520|\n",
      "|          Bill Navvy|              1|            0|              0|             15|\n",
      "|    Bolu Oluwagbesan|              1|            0|              1|          21689|\n",
      "|  MILF & SHEMALE FUN|              1|           52|              0|            299|\n",
      "|  PeopleSearches.com|              1|           21|              0|         362572|\n",
      "|              Mary D|              1|          221|              0|            159|\n",
      "|üëëüå∏ ·õï·ó©·ó©·í™·é•·ëé ·õï·ó¥·ëé∆≥·ó¥...|              1|          110|              0|            366|\n",
      "|         Alex Merola|              1|            0|              0|           1053|\n",
      "|              Kashif|              1|            0|              0|             17|\n",
      "|                   K|              1|            0|              0|            108|\n",
      "|           S Proctor|              1|         1472|              0|            275|\n",
      "|        Moya Johnson|              1|            1|              0|            835|\n",
      "|          John Avixa|              1|            1|              0|            223|\n",
      "|         Moosehammer|              1|            0|              1|            946|\n",
      "|                Arun|              1|            0|              0|            388|\n",
      "|       Mazen Alsaleh|              1|            1|              0|             76|\n",
      "+--------------------+---------------+-------------+---------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df4.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------------------------+------------------+\n",
      "|    name|first(favorite_count, false)|avg(retweet_count)|\n",
      "+--------+----------------------------+------------------+\n",
      "| Ronwood|                           0|           11750.0|\n",
      "|Mr. V...|                           0|          5713....|\n",
      "|    Hina|                           0|            1401.7|\n",
      "| APERIR√â|                           0|               0.0|\n",
      "|Amber...|                           0|               6.0|\n",
      "+--------+----------------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#DATE(FROM_UNIXTIME(created_at)) AS created_at,\n",
    "query = '''\n",
    "SELECT user.name, first(favorite_count), avg(retweet_count) FROM tweetView group by user.name\n",
    "'''\n",
    "spark.sql(query).show(5,truncate=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('''SELECT full_text from tweetView where possibly_sensitive is not null and possibly_sensitive = true and lang = 'en' and lower(full_text) rlike 'social distancing|who|cdc' limit 10''').show(10,False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://bucket-covid/TweetData/COVID-19-TweetIDs-master/2020-01/coronavirus-tweet-id-2020-01-21-22.jsonl.gz\n",
      "gs://bucket-covid/TweetData/COVID-19-TweetIDs-master/2020-01/coronavirus-tweet-id-2020-01-21-23.jsonl.gz\n"
     ]
    }
   ],
   "source": [
    "!gsutil ls \"gs://bucket-covid/TweetData/COVID-19-TweetIDs-master/2020-01/coronavirus-tweet-id-2020-01-21*.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sourceData2 = spark.read.format(\"json\").load(\"gs://bucket-covid/TweetData/COVID-19-TweetIDs-master/2020-01/coronavirus-tweet-id-2020-01-21-22.jsonl.gz\")\n",
    "sourceData2.repartition(10).write.save(\"parquetFile2.parquet\")\n",
    "parquetFile2 = spark.read.parquet(\"parquetFile2.parquet\")\n",
    "parquetFile2.createOrReplaceTempView(\"tweetView2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-cpu.2-1.m46",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-cpu.2-1:m46"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
